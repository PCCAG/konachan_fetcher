## coding=utf-8
import asyncio
import concurrent.futures
import multiprocessing
import os
import random

# import sys

import pandas

# from queue import Queue
import tqdm
from dotenv import load_dotenv
from loguru import logger  # pip install loguru

from cellfunctions_ import (
    Counter,
    ensure_file_exists,  # counter_sync,
    get_source,
    parse,
    read_headers_from_json,
    save_img,
    count_rechange_files_directory,
)

# from tqdm.contrib.concurrent import process_map

# åŠ è½½é…ç½®æ–‡ä»¶
load_dotenv()
logger.add("k_spider/log/get_imags_data.log")
# proxies=None

# ä¸»å‡½æ•°
# 3ä¸ªå¤§ä»»åŠ¡ä¾æ¬¡ä¼ é€’å‚æ•°
# è¿™é‡Œå¯ä»¥ç”¨é˜Ÿåˆ—è¿›ä¸€æ­¥æ”¹,æ‡’å¾—æ”¹äº†ğŸ˜…,æˆ‘çœ‹å¾—çœ¼èŠ±
# å¾—åˆ°æ•°æ®

# logger.disable("ERROR")
# logger.add(sys.stdout, level="INFO", enqueue=True)


@logger.catch()
def main(
    pids: list[int],
    sem_times: int = int(os.getenv("sem_times")),  # å¹¶å‘æ•° åœ¨.envé…ç½®
    imgpath: str = os.getenv("IMG_PATH"),  # ä¿å­˜å›¾ç‰‡è·¯å¾„,åœ¨.envé…ç½®
    headers: dict = read_headers_from_json(),  # è¯·æ±‚å¤´
) -> list | list[tuple[int, str, str, str]]:  # [(pid,tags,img_path,img_link),....]
    #
    #

    imgpath = count_rechange_files_directory(imgpath)

    #
    #

    async def process_urls(pids: list[int]) -> set[tuple[int, str]]:
        sem = asyncio.Semaphore(sem_times)  # å¹¶å‘æ•°
        get_source_counter = Counter.counter_async(
            PROCESS_BAR=tqdm.tqdm(
                total=len(pids),
                desc="è·å–æºç ",
                unit="i",
                smoothing=0.5,
            )
        )(get_source)
        async with sem:
            tasks = {
                asyncio.create_task(
                    get_source_counter(
                        pid, f"https://konachan.com/post/show/{pid}/", headers
                    )
                )
                for pid in pids
            }

            r = await asyncio.gather(*tasks, return_exceptions=False)
            # å‚æ•°
            results = {
                (i, ii) for i, ii in r if ii != "å¯„"
            }  # pid_source=[(pid,source),......]
            return results

    # pid_source=[(pid,source),......]
    # tqdm.tqdm.write("")
    pid_source = asyncio.run(process_urls(pids))
    # tqdm.tqdm.write("")

    # PROCESS_BAR.close()
    if len(pid_source) == 0:
        return []  # ç¬¬ä¸€æ­¥æ²¡æœ‰ä¸œè¥¿åé¢ä¹Ÿå°±æ²¡æœ‰å¿…è¦ç»§ç»­äº†

    # å¤šè¿›ç¨‹è§£æ
    # ğŸ˜…
    def run_tasks(task, pid_source: set[tuple[int, str]]):
        max_workers = min(len(pid_source), multiprocessing.cpu_count())
        # results = set()  # ç»“æœé›†åˆ
        with concurrent.futures.ProcessPoolExecutor(
            max_workers=max_workers
        ) as executor:
            futures = {
                executor.submit(task, pid, source)
                for pid, source in pid_source
                if source != "å¯„"
            }

            results = set()
            for future in tqdm.tqdm(
                concurrent.futures.as_completed(futures),
                total=len(pid_source),
                desc="è§£æåœ°å€",
                unit="i",
                smoothing=0.5,
            ):
                results.add(future.result())
        return results

    pid_img_link_tags = run_tasks(parse, pid_source)
    if len(pid_img_link_tags) == 0:
        return []  # ç¬¬äºŒæ­¥æ²¡æœ‰ä¸œè¥¿åé¢ä¹Ÿå°±æ²¡æœ‰å¿…è¦ç»§ç»­äº†

    # å¹¶å‘ä¸‹è½½å›¾ç‰‡
    async def download_imgs(pid_img_link_tags: set[tuple[int, str, str]]):
        sem = asyncio.Semaphore(sem_times)  # å¹¶å‘æ•°
        save_img_counter = Counter.counter_async(
            tqdm.tqdm(
                total=len(pid_img_link_tags),
                desc="ä¸‹è½½å›¾ç‰‡",
                unit="i",
                smoothing=0.5,
            )
        )(save_img)
        async with sem:
            tasks = {
                asyncio.create_task(
                    save_img_counter(pid, img_link, tags, headers, imgpath)
                )
                for pid, img_link, tags in pid_img_link_tags
            }
            results = await asyncio.gather(*tasks)

            results = [
                (pid, tags, img_path, img_link)
                for pid, tags, img_path, img_link in results
                if tags != "å¯„" and img_link != "å¯„" and img_path != "å¯„"
            ]

            logger.success(f"downloaded {len(results)} img to IMG_PATH......")
            return results

    pid_tags_img_link_img_path = asyncio.run(download_imgs(pid_img_link_tags))

    return pid_tags_img_link_img_path
    # è¿”å›kimgè¡¨çš„å¾ˆå¤šè¡Œçš„ä¿¡æ¯


# example test
if __name__ == "__main__":
    # logger.disable("ERROR")
    # logger.add(sys.stdout, level="INFO", enqueue=True)
    # ç§»é™¤æ‰€æœ‰è¾“å‡ºç›®æ ‡ï¼Œç¦ç”¨æ‰€æœ‰æ—¥å¿—è¾“å‡º
    if int(os.getenv("EnableLog")) == 0:
        logger.remove()
        print("å…³é—­æ—¥å¿—")
    else:
        print("æ‰“å¼€æ—¥å¿—")
        logger.disable("SUCCESS")
        # logger.s
        # logger.disable("ERROR")
        # logger.disable("WARNING")
        # logger.disable("DEBUG")

        pass
    for _ in range(int(os.getenv("times"))):
        pids = random.sample(
            list(range(int(os.getenv("low")), int(os.getenv("upper")) + 1)),
            int(os.getenv("down_number")),
        )
        # print(f"å•æ¬¡å¹¶å‘æ•°é‡: {pids.__len__()}")
        # headers = read_headers_from_json()

        rows = main(pids)

        nowtime = pandas.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")

        data = [
            (pid, tags, img_link, img_path, 1, nowtime)
            for pid, tags, img_link, img_path in rows
        ]

        df = pandas.DataFrame(
            columns=("pid", "tags", "link", "path", "status", "time"), data=data
        )

        # è‡ªå®šä¹‰å‡½æ•°ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        if ensure_file_exists("./Data/kimg.csv") == False:
            df.to_csv("./Data/kimg.csv", index=False, mode="w")
        ensure_file_exists("./Data/tags.csv")

        df.to_csv(
            "./Data/kimg.csv",
            index=False,
            mode="a",
            header=False,
        )
        # ./Data/tags.csv

        row_tags: list = df["tags"].to_list()

        tags_set: set = set()
        # for tags in row_tags:
        #     for tag in tags.split(" "):
        #         tags_set.add(tag)
        # ä½¿ç”¨å†…ç½®çš„é›†åˆæ“ä½œ
        tags_set.update(tag for tags in row_tags for tag in tags.split(" "))

        try:
            alltags_existed_set: set = set(
                pandas.read_csv("./Data/tags.csv").iloc[:, 0].to_list()
            )
        except Exception as e:
            alltags_existed_set: set = set()
            pass

        need_add_tags: list = list(tags_set - alltags_existed_set)

        # è¿½åŠ è¿›å»çš„æ˜¯åº”è¯¥ä¸ºåŸè¡¨ä¸­ä¸å­˜åœ¨çš„

        pandas.DataFrame(data=need_add_tags, index=None, columns=None).to_csv(
            "./Data/tags.csv", mode="a", index=False, header=False
        )

        # with open("./Data/tags.csv", "a", encoding="UTF-8") as f:
        #     f.writelines()
