## coding=utf-8
import asyncio
import functools

# import json
import os
import random

# import re as re__

# import types

# å¼‚æ­¥ä¿å­˜æ–‡ä»¶çš„åº“
# pip install aiofiles
import aiofiles
import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fake_useragent import UserAgent
from loguru import logger

# from playwright.sync_api import sync_playwright  # , Playwright

# ä¸€ä¸ªplaywrightçš„æ’ä»¶ç”¨æ¥ä¼ªè£…æ— å¤´æ¨¡å¼
# pip install playwright-stealth
# from playwright_stealth import stealth_sync

from parse_tags_link import f_link, f_tags


# åŠ è½½é…ç½®æ–‡ä»¶
load_dotenv()


##kç«™ä¸»é¡µ
host = "https://konachan.com"

# # åˆ©ç”¨clashä»£ç†è½¬å‘
# proxies = {"http://": "http://127.0.0.1:10809", "https://": "http://127.0.0.1:10809"}
http_proxy = os.getenv("http_proxy")

proxies = {"http://": http_proxy, "https://": http_proxy}

# logger.warning(f"ä»£ç†: {http_proxy}")

# # httpxçš„socks5ä»£ç†å¥½åƒæœ‰é—®é¢˜
# socks5_proxies = {
#     "http://": "socks5://127.0.0.1:62333",
#     "https://": "socks5://127.0.0.1:62333",
# }

# proxies=None


# def read_headers_from_json(
#     cookie_json_filepath="_cookies_.json", headers_example="_headers_example.json"
# ) -> dict:
#     """
#     é»˜è®¤:
#     ä» headers_example="_headers_ example.json" è·å–headersæ¨¡æ¿,
#     ä» cookie_json_filepath="cookies.json" æ›´æ–°headersçš„cookieå­—æ®µ,
#     å¦‚æœå¤±è´¥è°ƒç”¨get_headers(),
#     è¿”å›: headers
#     """

#     # logger.debug("å°è¯•ä»cookies.jsonè·å–headers çš„cookie.......")
#     with open(cookie_json_filepath, encoding="utf-8") as f:
#         try:
#             # dread = dict(json.load(f))
#             d_list: list = json.load(f)
#             # dict().keys()
#             # print(list(dread.keys())[0])
#             headers_cookie = "; ".join([f'{d["name"]}={d["value"]}' for d in d_list])
#             with open(headers_example, encoding="utf-8") as h:
#                 headers = dict(json.load(h))
#                 headers["cookie"] = headers_cookie
#             # print(headers)
#         except Exception as e:
#             # raise e
#             # raise NameError("Not found headers") from e
#             logger.warning(
#                 "å°è¯•ä»cookies.jsonè·å–headers çš„cookieå¤±è´¥,å°†ç”¨å‡½æ•°get_headers()é‡‡ç”¨playwrightæ— å¤´æ¨¡å¼ä¸‹è‡ªåŠ¨è·å–headers"
#             )
#             headers = get_headers()
#             # headers = {}

#         # logger.success("è·å–headersæˆåŠŸ......")

#         return headers


# # ä¸€ä¸ªè·å–å½“å‰ipçš„å‡½æ•°
# async def current_ip():
#     """
#     ä¸€ä¸ªè·å–å½“å‰ipçš„å‡½æ•°
#     """
#     async with httpx.AsyncClient() as c:
#         try:
#             re = await c.get(
#                 "http://whois.pconline.com.cn/ipJson.jsp", params={"json": "true"}
#             )
#             return re.json() if len(re.json()["ip"]) > 4 else {"ip": "æ²¡æœ‰è·å–åˆ°"}
#         except Exception as e:
#             raise e


# # åˆ©ç”¨playwrightè·å–cookieåŠheaders
# @logger.catch()
# def get_headers() -> dict[str, str]:
#     """
#     åˆ©ç”¨playwrightè·å–cookieåŠheaders
#     """
#     # å¯åŠ¨æ¼”å¥
#     with sync_playwright() as p:
#         try:
#             logger.debug("åˆ©ç”¨playwrightè·å–cookieåŠheaders......")
#             # å¯åŠ¨æµè§ˆå™¨
#             b = p.webkit.launch(
#                 headless=True, proxy={"server": proxies[list(proxies.keys())[0]]}  # type: ignore
#             )
#             logger.debug("å¯åŠ¨æµè§ˆå™¨ä¸Šä¸‹æ–‡....")
#             # å¯åŠ¨æµè§ˆå™¨ä¸Šä¸‹æ–‡
#             context = b.new_context()

#             page = context.new_page()
#             # æ— å¤´æ¨¡å¼ä¼ªè£…
#             stealth_sync(page)

#             # è·¯ç”±,æµäº§å›¾ç‰‡,å®ç°æ— å›¾æ¨¡å¼
#             page.route(
#                 re__.compile(r"(\.png$)|(\.jpg$)|(\.gif$)", re__.S),
#                 lambda route: route.abort(),
#             )

#             re = page.goto(host)
#             page.click("#links > a:nth-child(1)")

#             page.wait_for_load_state("domcontentloaded", timeout=10000)
#             page.click("#post-list-posts > li > div > a")

#             page.wait_for_load_state("domcontentloaded", timeout=10000)

#             # è·å–æ‰€æœ‰cookie
#             cookies = context.cookies()
#             # è·å–è¯·æ±‚å¤´
#             headers = re.request.headers  # type: ignore
#             headers["cookie"] = "".join(
#                 (
#                     f"{i['name']}={i['value']};"
#                     if pin != len(cookies) - 1
#                     else f"{i['name']}={i['value']}"
#                 )
#                 for pin, i in enumerate(cookies)
#             )
#             headers["Referer"] = r"https://konachan.com/post"

#             with open("_headers_.json", "w", encoding="UTF-8", errors="ignore") as f:
#                 json.dump(headers, fp=f, ensure_ascii=False, indent=2)

#             page.close()
#             context.close()
#             b.close

#             logger.success("è·å–cookieåŠheadersæˆåŠŸ")

#             return headers
#         except Exception as e:
#             logger.error("å–cookieåŠheaderså¤±è´¥,è¯·å°è¯•æ£€æŸ¥ä»£ç†æ˜¯å¦å‡ºé”™")
#             breakpoint()
#             raise e


class Counter:
    def __init__(self) -> None:
        pass

    # é»˜è®¤
    class PROCESS_BAR_NONE:
        def update(*arg, **kwargs):
            pass

    @staticmethod
    def counter_async(PROCESS_BAR=PROCESS_BAR_NONE()):
        """
        å‚æ•°è£…é¥°å™¨ : ä¼ å…¥PROCESS_BAR å®ç°å¯¹åŸå‡½æ•°è¿›åº¦æ§åˆ¶.
        ä¾‹å¦‚: PROCESS_BAR = tqdm.tqdm(total=200, desc="Processing", unit="item"),
        tqdm.tqdmè¿›åº¦æ¡(import tqdm),
        éœ€è¦åœ¨å…¶ä»–æ–‡ä»¶å¯¼å…¥å¹¶è¦†å†™åŸå¼‚æ­¥å‡½æ•°,
        newfunc = counter_async(PROCESS_BAR=tqdm.tqdm(total=200, desc="Test", unit="item"))(example)
        """
        ##PROCESS_BAR = tqdm.tqdm()

        def async_counter(func):
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                result = await func(*args, **kwargs)
                wrapper.processcount.update(1)  # type: ignore
                if wrapper.processcount.n == wrapper.processcount.total:  # type: ignore
                    wrapper.processcount.close()  # type: ignore

                return result

            wrapper.processcount = async_counter.processcount  # type: ignore
            return wrapper

        async_counter.processcount = PROCESS_BAR  # type: ignore
        return async_counter

    @staticmethod
    def counter_sync(PROCESS_BAR=PROCESS_BAR_NONE()):
        """
        ä¸æ”¯æŒå¤šè¿›ç¨‹ğŸ˜…
        """

        def sync_counter(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                r = func(*args, **kwargs)
                wrapper.processcount.update(1)  # type: ignore
                if wrapper.processcount.n == wrapper.processcount.total:  # type: ignore
                    wrapper.processcount.close()  # type: ignore
                return r

            wrapper.processcount = sync_counter.processcount  # type: ignore
            return wrapper

        sync_counter.processcount = PROCESS_BAR  # type: ignore
        return sync_counter


async def get_source(
    pid: int, url: str, headers: dict, proxies: dict = proxies
) -> tuple[int, str]:
    """
    è·å–å•ä¸ªé¡µé¢æºç 
    return: pid, source

    """
    # sourcery skip: remove-unreachable-code
    # å¼‚æ­¥ä¸Šä¸‹æ–‡èœå•ç®¡ç†å™¨
    async with httpx.AsyncClient(proxies=proxies) as client:
        # è¿™ä¸ªæ˜¯é‡è¯•å››æ¬¡,2333ğŸ¤£
        for i in range(4):
            try:
                headers["user-agent"] = UserAgent().random
                re = await client.get(url, headers=headers, timeout=(i + 1) * 50)
                res = re.text
                if "<p>This post does not exist.</p>" in res or (
                    "This post was deleted." in res
                    and """<div class="status-notice">""" in res
                ):
                    logger.error(f"{pid}å›¾ç‰‡ä¸å­˜åœ¨æˆ–è€…è¢«åˆ é™¤äº†")
                    logger.error(f"https://konachan.com/post/show/{pid}/")
                    return (pid, "å¯„")
                elif len(res) < 2000:
                    logger.warning("æºç é•¿åº¦ä¸å¯¹")
                    await asyncio.sleep(random.randint(1, 4))
                    logger.warning(f"https://konachan.com/post/show/{pid}/")
                    continue
                elif re.status_code == 403:
                    logger.error("403æ— æ³•è®¿é—®")
                    return (pid, "å¯„")
                elif re.status_code == 200:
                    logger.success("è·å–æºç æˆåŠŸ")
                    return (pid, res)
            except Exception as e:
                # raise e
                logger.warning(f"ä¸‹è½½æºç é‡è¯•: https://konachan.com/post/show/{pid}")
                await asyncio.sleep(random.randint(1, 4))
                # raise e
                # lg.warning("re link")
                continue
        logger.error("æºç  nothing!")
        logger.error(f"https://konachan.com/post/show/{pid}/")
        return (pid, "å¯„")


# è§£ææºç å¾—åˆ°å›¾ç‰‡åœ°å€å’Œtags
# @counter
def parse(pid: int, source: str) -> tuple[int, str, str]:
    """
    è§£ææºç å¾—åˆ°å›¾ç‰‡åœ°å€å’Œtags.
    return: pid,tags,img_link
    """
    # sourcery skip: remove-redundant-continue, remove-unnecessary-else
    # global df_error
    try:
        tags_link = [
            pid,
        ]
        # æ‰€æœ‰è§£æå™¨è¯•ä¸€é.23333ğŸ¤£
        for param_features in (
            "lxml",
            "html.parser",
            "html5lib",
            "html",
            "html5",
            "xml",
            "lxml-xml",
        ):
            soup = BeautifulSoup(source, features=param_features)
            tags = f_tags(soup)
            link = f_link(soup)
            if (tags != False) and (link != False):
                tags_link.extend((link, tags))  # type: ignore
                break
            else:
                continue
        if len(tags_link) == 3:
            return tuple(tags_link)  # type: ignore
        else:
            logger.error(f"è§£æé”™è¯¯last: https://konachan.com/post/show/{pid}/")
            return (pid, "å¯„", "å¯„")
    except Exception as e:
        logger.error(f"è§£æé”™è¯¯all:  https://konachan.com/post/show/{pid}/")
        return (pid, "å¯„", "å¯„")
        # raise e


# ä¸‹è½½å›¾ç‰‡
# @counter
async def save_img(
    pid: int,
    img_link: str,
    tags: str,
    headers: dict,
    imgpath: str,
    proxies: dict = proxies,
) -> tuple[int, str, str, str]:
    """
    ä¸‹è½½å›¾ç‰‡.
    è¿”å›: return (pid, tags, img_link, img_path)
    """

    async def check_ifsuccess_return_responce():
        async with httpx.AsyncClient(proxies=proxies) as clinet:
            for _ in range(3):
                try:
                    headers["user-agent"] = UserAgent().random
                    re = await clinet.get(img_link, headers=headers, timeout=300)
                    assert re.status_code == 200, "è¯·æ±‚å›¾ç‰‡çŠ¶æ€ç :{re.status_code}"
                    return re
                except Exception:
                    logger.warning(
                        f"ä¸‹è½½å›¾ç‰‡é‡è¯•: https://konachan.com/post/show/{pid}"
                    )
                    await asyncio.sleep(random.randint(1, 4))
                    continue
            return False

    try:
        re = await check_ifsuccess_return_responce()

        # è¿™é‡Œéå¼‚æ­¥

        if re == False:
            logger.error("è¯·æ±‚å›¾ç‰‡é“¾æ¥é”™è¯¯")
            logger.error(f"https://konachan.com/post/show/{pid}/")
            return (pid, "å¯„", "å¯„", "å¯„")
        # logger.debug(f"è¯·æ±‚å›¾ç‰‡çŠ¶æ€ç :{re.status_code}")

        elif re.status_code != 200:
            logger.error(f"è¯·æ±‚å›¾ç‰‡çŠ¶æ€ç :{re.status_code}")
            logger.error(f"https://konachan.com/post/show/{pid}/")
            return (pid, "å¯„", "å¯„", "å¯„")
        else:
            img_type = img_link.split(".")[-1]
            # logger.info(f"å›¾ç‰‡ç±»å‹: {img_type}")

            # å›¾ç‰‡è·¯å¾„
            img_path = f"{imgpath}/{pid}.{img_type}"

            # logger.info(f"å›¾ç‰‡æµç±»å‹: {type(re.content)}") äºŒè¿›åˆ¶

            # ä½¿ç”¨å¼‚æ­¥å†™å…¥æ–‡ä»¶

            async with aiofiles.open(img_path, "wb") as f:
                await f.write(re.content)
                # logger.debug(f"ä¿å­˜å›¾ç‰‡ä¸­......{pid}")

            if tags != "å¯„" and img_link != "å¯„" and img_path != "å¯„":
                logger.success(f"ä¸‹è½½å›¾ç‰‡æˆåŠŸ: {img_path}")
                return (pid, tags, img_link, img_path)

            else:
                logger.error("æ²¡æœ‰æœ‰æ•ˆçš„å€¼,ä¸‹è½½å›¾ç‰‡å¤±è´¥")

                logger.error(f"https://konachan.com/post/show/{pid}/")

                return (pid, "å¯„", "å¯„", "å¯„")

    except Exception as e:
        logger.error("ä¿å­˜å›¾ç‰‡åˆ°è·¯å¾„é”™è¯¯")

        logger.error(f"https://konachan.com/post/show/{pid}/")

        breakpoint()

        return (pid, "å¯„", "å¯„", "å¯„")


# ğŸ˜…
def ensure_file_exists(filepath, file_ecoding="UTF-8") -> bool:  # type: ignore
    """
    æ£€æµ‹æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–‡ä»¶ã€‚

    å‚æ•°ï¼š
    - filepath: æ–‡ä»¶è·¯å¾„
    - file_ecoding: åˆ›å»ºçš„æ–‡ä»¶ç¼–ç 
    return:
    Ture è¿™ä¸ªæ–‡ä»¶å­˜åœ¨
    False åä¹‹
    """
    try:
        with open(filepath, "r", errors="ignore"):
            pass
        return True
    except FileNotFoundError:
        # os.makedirs("/".join(filepath.split("/")[:-1]))
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, "w", errors="ignore", encoding=file_ecoding):
            pass
        return False
    except Exception:
        print("constom function:  ensure_file_exists,  raise an unknown error ! ?")


# # æ£€æµ‹å¹¶åˆ›å»ºæ–‡ä»¶è·¯å¾„
# file_path = r"path\to\your\file.txt"
# ensure_file_exists(file_path)
